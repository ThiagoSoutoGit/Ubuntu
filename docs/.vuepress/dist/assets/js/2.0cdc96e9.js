(window.webpackJsonp=window.webpackJsonp||[]).push([[2],{323:function(t,s,a){t.exports=a.p+"assets/img/DeepNet14.2a25a87a.png"},324:function(t,s,a){t.exports=a.p+"assets/img/DeepNet15.4c7ec8bd.png"},325:function(t,s,a){t.exports=a.p+"assets/img/DeepNet19.d0225ce2.png"},326:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation9.611be532.png"},352:function(t,s,a){t.exports=a.p+"assets/img/TrainingData-Example_1.167f9cbf.svg"},353:function(t,s,a){t.exports=a.p+"assets/img/LastOutputvsImputs-Example_1.cecc4123.svg"},354:function(t,s,a){t.exports=a.p+"assets/img/Tensorflow_example1graph.5c547a49.png"},355:function(t,s,a){t.exports=a.p+"assets/img/INputOutp.d6d85074.png"},356:function(t,s,a){t.exports=a.p+"assets/img/notlinear.e6eda647.png"},357:function(t,s,a){t.exports=a.p+"assets/img/layer.30532299.png"},358:function(t,s,a){t.exports=a.p+"assets/img/DeepNet.8bf766f6.png"},359:function(t,s,a){t.exports=a.p+"assets/img/DeepNet2.e44593c1.png"},360:function(t,s,a){t.exports=a.p+"assets/img/DeepNet3.638db6a6.png"},361:function(t,s,a){t.exports=a.p+"assets/img/DeepNet4.ac034b86.png"},362:function(t,s,a){t.exports=a.p+"assets/img/DeepNet5.5a84a0a6.png"},363:function(t,s,a){t.exports=a.p+"assets/img/DeepNet6.6010cb9d.png"},364:function(t,s,a){t.exports=a.p+"assets/img/DeepNet7.f5683ee2.png"},365:function(t,s,a){t.exports=a.p+"assets/img/DeepNet8.cea37426.png"},366:function(t,s,a){t.exports=a.p+"assets/img/DeepNet9.e9d71476.png"},367:function(t,s,a){t.exports=a.p+"assets/img/DeepNet10.e92be7af.png"},368:function(t,s,a){t.exports=a.p+"assets/img/DeepNet11.92c12397.png"},369:function(t,s,a){t.exports=a.p+"assets/img/DeepNet12.3ff8c451.png"},370:function(t,s,a){t.exports=a.p+"assets/img/DeepNet13.1294dbf9.png"},371:function(t,s,a){t.exports=a.p+"assets/img/DeepNet16.d67f86c9.png"},372:function(t,s,a){t.exports=a.p+"assets/img/DeepNet17.07c24da2.png"},373:function(t,s,a){t.exports=a.p+"assets/img/DeepNet18.8ff108ee.png"},374:function(t,s,a){t.exports=a.p+"assets/img/Softmax1.a9860eb4.png"},375:function(t,s,a){t.exports=a.p+"assets/img/Softmax2.dd367e1d.png"},376:function(t,s,a){t.exports=a.p+"assets/img/Softmax3.2baf07f3.png"},377:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation1.f4403676.png"},378:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation2.62b3c17c.png"},379:function(t,s,a){t.exports=a.p+"assets/img/GradientDescent.4e73561c.png"},380:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation3.665e3a7d.png"},381:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation4-Forward.f6e69472.png"},382:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation5.de199fcb.png"},383:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation6.6628b3bb.png"},384:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation7.7aba3a0e.png"},385:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation8.e7cb69ec.png"},386:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation10.7c48d985.png"},387:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation11.8021980a.png"},388:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation12.65bdcdac.png"},389:function(t,s,a){t.exports=a.p+"assets/img/Backpropagation13.2350c7d3.png"},390:function(t,s,a){t.exports=a.p+"assets/img/FitModel.0f38b4f1.png"},391:function(t,s,a){t.exports=a.p+"assets/img/Underfitted.ef9bf9cb.png"},392:function(t,s,a){t.exports=a.p+"assets/img/Overfitted.7e586205.png"},393:function(t,s,a){t.exports=a.p+"assets/img/AlthreeFitModels.08ddbf8a.png"},394:function(t,s,a){t.exports=a.p+"assets/img/TrainingValidationAndTest.dff55a6a.png"},395:function(t,s,a){t.exports=a.p+"assets/img/Training.840b8310.png"},396:function(t,s,a){t.exports=a.p+"assets/img/Validation.5e34f023.png"},397:function(t,s,a){t.exports=a.p+"assets/img/TrainingandValidation.bedacac2.png"},398:function(t,s,a){t.exports=a.p+"assets/img/OverfittingFlag.b183ee04.png"},399:function(t,s,a){t.exports=a.p+"assets/img/ValidationLossVsTrainingLoss.beee1c35.png"},403:function(t,s,a){"use strict";a.r(s);var n=a(43),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"tensorflow"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tensorflow","aria-hidden":"true"}},[t._v("#")]),t._v(" TensorFlow")]),t._v(" "),n("h2",{attrs:{id:"simple-linear-regression"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#simple-linear-regression","aria-hidden":"true"}},[t._v("#")]),t._v(" Simple Linear Regression.")]),t._v(" "),n("h3",{attrs:{id:"minimal-example"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#minimal-example","aria-hidden":"true"}},[t._v("#")]),t._v(" Minimal Example")]),t._v(" "),n("ol",[n("li",[t._v("Import the relevant libraries")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" mpl_toolkits"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mplot3d "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Axes3D\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br")])]),n("ol",{attrs:{start:"2"}},[n("li",[t._v("Generate random input data to train on")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("observations "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),t._v("\nxs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("low"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" high"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nzs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ninputs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("column_stack"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br")])]),n("p",[t._v("Output is a matrix of size (1000,2)")]),t._v(" "),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("(1000, 2)\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("[[ 7.44651066  3.0441044 ]\n [ 3.18741031 -6.10663328]\n [ 7.47234553  6.86829353]\n ...\n [ 4.3408767   2.59859389]\n [ 5.96692549 -1.95235124]\n [ 6.43664934 -8.52279315]]\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br")])]),n("table",[n("thead",[n("tr",[n("th",{staticStyle:{"text-align":"center"}},[t._v("Elements of the model in supervised learning")]),t._v(" "),n("th",{staticStyle:{"text-align":"center"}},[t._v("Status")])])]),t._v(" "),n("tbody",[n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("inputs")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("done")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("weights")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("Computer")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("biases")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("Computer")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("outputs")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("Computer")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("targets")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v("to do")])])])]),t._v(" "),n("p",[t._v("Targets = $f(x,z) = 2x - 3z + 5 + noise$")]),t._v(" "),n("p",[t._v("Where 2 is the first weight$(W_1)$ 3 is the second weight$(W_2)$ and 5$(b)$ is the baias.")]),t._v(" "),n("p",[t._v("The noise is introduced to randomize the data.")]),t._v(" "),n("ol",{attrs:{start:"3"}},[n("li",[t._v("Create the targets we will aim at")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("noise "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntargets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("xs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("zs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" noise\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the targets are a linear combination of two vectors 1000x1 ")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a scalar and noise 1000x1, their shape should be 1000x1")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br")])]),n("p",[t._v("Output the shape of the targets, a matrix 1000x1")]),t._v(" "),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("(1000, 1)\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("ol",{attrs:{start:"4"}},[n("li",[t._v("Plot the training data")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.")]),t._v("\ntargets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Plotting according to the conventional matplotlib.pyplot syntax")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Declare the figure")]),t._v("\nfig "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("figure"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# A method allowing us to create the 3D plot")]),t._v("\nax "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fig"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_subplot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("111")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" projection"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'3d'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Choose the axes.")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Set labels")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_xlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_ylabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'zs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_zlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# to azim = 0 ; azim = 200, or whatever. Check and see what happens.")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view_init"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("azim"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# So far we were just describing the plot. This method actually shows the plot. ")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We reshape the targets back to the shape that they were in before plotting.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# This reshaping is a side-effect of the 3D plot. Sorry for that.")]),t._v("\ntargets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br"),n("span",{staticClass:"line-number"},[t._v("11")]),n("br"),n("span",{staticClass:"line-number"},[t._v("12")]),n("br"),n("span",{staticClass:"line-number"},[t._v("13")]),n("br"),n("span",{staticClass:"line-number"},[t._v("14")]),n("br"),n("span",{staticClass:"line-number"},[t._v("15")]),n("br"),n("span",{staticClass:"line-number"},[t._v("16")]),n("br"),n("span",{staticClass:"line-number"},[t._v("17")]),n("br"),n("span",{staticClass:"line-number"},[t._v("18")]),n("br"),n("span",{staticClass:"line-number"},[t._v("19")]),n("br"),n("span",{staticClass:"line-number"},[t._v("20")]),n("br"),n("span",{staticClass:"line-number"},[t._v("21")]),n("br"),n("span",{staticClass:"line-number"},[t._v("22")]),n("br"),n("span",{staticClass:"line-number"},[t._v("23")]),n("br"),n("span",{staticClass:"line-number"},[t._v("24")]),n("br"),n("span",{staticClass:"line-number"},[t._v("25")]),n("br"),n("span",{staticClass:"line-number"},[t._v("26")]),n("br"),n("span",{staticClass:"line-number"},[t._v("27")]),n("br"),n("span",{staticClass:"line-number"},[t._v("28")]),n("br"),n("span",{staticClass:"line-number"},[t._v("29")]),n("br"),n("span",{staticClass:"line-number"},[t._v("30")]),n("br")])]),n("p",[t._v("Clean code")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("targets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfig "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("figure"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fig"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_subplot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("111")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" projection"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'3d'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_xlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_ylabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'zs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_zlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nax"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view_init"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("azim"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntargets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br")])]),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(352),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("ol",{attrs:{start:"5"}},[n("li",[t._v("Create weights")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# our initial weights and biases will be picked randomly from")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the interval minus 0.1 to 0.1.")]),t._v("\ninit_range "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#The size of the weights matrix is two by one as we")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# have two variables so there are two weights one for#")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#each input variable and a single output.")]),t._v("\nweights "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("init_range"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" init_range"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Weights: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br")])]),n("ol",{attrs:{start:"6"}},[n("li",[t._v("Create Biases")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Let's declare the bias and illogically the appropriate shape is one by one.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#So the bias is a scalar in machine learning.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#There are many biases as there are outputs.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Each bias refers to an output.")]),t._v("\nbiases "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("init_range"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" init_range"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Biases: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("biases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br")])]),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("Weights: [[-0.07021836]\n [ 0.00626743]]\nBiases: [-0.01464248]\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br")])]),n("ol",{attrs:{start:"7"}},[n("li",[t._v("Set a learning rate")])]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("learning_rate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.02")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("p",[t._v("So we are all set.\nWe have inputs targets and arbitrary numbers for weights and biases.\nWhat is left is to vary the weights and biases so our outputs are closest to the targets as we know by now.\nThe problem boils down to minimizing the loss function with respect to the weights and the biases.\nAnd because this is a regression we'll use one half the L2 norm loss function.")]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Next let's make our model learn.")]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Since this is an iterative problem.\nWe must create a loop which will apply our update rule and calculate the last function.")]),t._v(" "),n("p",[t._v("I'll use a for loop with 100 iterations to complete this task.\nLet's see the game plan will follow:")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("At each iteration We will calculate the outputs")])]),t._v(" "),n("li",[n("p",[t._v("and compare them to the targets through the last function.")])]),t._v(" "),n("li",[n("p",[t._v("We will print the last for each iteration so we know how the algorithm is doing.")])]),t._v(" "),n("li",[n("p",[t._v("Finally we will adjust the weights and biases to get a better fit of the data.")])]),t._v(" "),n("li",[n("p",[t._v("At the next iteration these updated weights and biases will provide different outputs.")])]),t._v(" "),n("li",[n("p",[t._v("Then the procedure will be repeated.")])])]),t._v(" "),n("p",[t._v("Now the dot product of the input times the weights is 1000 by two times to buy one.\nSo a 1000 by 1 matrix when we add the bias which is a scalar.\nPython adds the element wise.\nThis means it is added to each element of the output matrix.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    outputs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" biases\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br")])]),n("p",[t._v("OK for simplicity let's declare a variable called deltas which will record the difference between the\noutputs and the targets.\nWe already introduce such variable in the gradient descent lecture deltas equals outputs minus targets.\nThat's useful as it is a part of the update rule.\nThen we must calculate the loss.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("deltas "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" outputs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" targets\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("br"),t._v(" "),n("p",[t._v("L2-norm loss formula:")]),t._v(" "),n("p",[t._v("$\\sum _i(y_i - t_i)^2$")]),t._v(" "),n("p",[t._v("We said we will use half the L2 norm loss.\nPython actually speaking deltas is a 1000 by one array.\nWe are interested in the sum of its terms squared.\nFollowing the formula for the L2 norm loss there is a num PI method called sum which will allow us to\nsum all the values in the array the L2 norm requires these values to be squared.\nSo the code looks like this.\nAnd P does some of Delta squared.\nWe then divide the whole expression by two to get the elegant update rules from the gradient descent.\nLet's further augment the loss by dividing it by the number of observations we have.\nThis would give us the average loss per observation or the mean loss.\nSimilarily to the division by 2.\nThis does not change the logic of the last function.\nIt is still lower than some more accurate results that will be obtained.\nThis little improvement makes the learning independent of the number of observations instead of adjusting\nthe learning rate.\nWe adjust the loss that that's valuable as the same learning rate should give us similar results for\nboth 1000 and 1 million observations.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("loss "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deltas "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" observations\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("p",[t._v("We'll print the last we've obtained each step.\nThat's done as we want to keep an eye on whether it is decreasing as iterations are performed.\nIf it is decreasing our machine learning algorithm functions well.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("p",[t._v("Finally we must update the weights and biases so they are ready for the next iteration using the same\nrescaling trick.\nI'll also reskill the deltas.\nThis is yet another way to make the algorithm more universal.\nSo the new variable is deltas underscored skilled and equals deltas divided by observations.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("deltas_scaled "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" deltas "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" observations\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("p",[t._v("Let's update the weights.\nWe will follow the gradient descent logic.")]),t._v(" "),n("p",[t._v("$w_{i+1} = w_i - \\eta \\sum _i x_i \\delta_i$")]),t._v(" "),n("p",[t._v("The new weights are equal to the old weights minus the learning rate times the dot product of the inputs\nand the Deltas underscored scaled.\nThe shape of the weights is two by one the shape of the inputs is one thousand by two and that of the\nDelta skilled is one thousand by one.\nObviously we cannot simply multiply the inputs and the deltas.\nThis is an issue that may arise occasionally due to the linear algebra involved to fix it.\nWe must transpose the inputs matrix using the object but the method.\nNow the major C's are compatible.\nBy 1000 times 1000 by one is equal to 2 by 1.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("weights "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weights "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" learning_rate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" deltas_scaled"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),n("p",[t._v("Often when dealing with matrices you find the correct way to code it through dimensionality checks and\ncompatability errors.")]),t._v(" "),n("p",[t._v("However transposing major C's doesn't affect the information they hold so we can do it freely.")])]),t._v(" "),n("p",[t._v("All right let's update the biases.\nThe new biases are equal to the old biases minus the learning rate times the sum of the deltas as explained\nin the gradient descent lecture.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[t._v("biases "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" biases "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" learning_rate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deltas_scaled"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("p",[t._v("This is the entire algorithm.\nLet's recap what it does:")]),t._v(" "),n("ol",[n("li",[t._v("first it calculates the outputs forgiven weights and biases.")]),t._v(" "),n("li",[t._v("Second it calculates a loss function that compares the outputs to the targets.")]),t._v(" "),n("li",[t._v("Third it prints the loss. So we can later analyze it and")]),t._v(" "),n("li",[t._v("forth, We update the weights and the bias is following the gradient descent methodology.")])]),t._v(" "),n("p",[t._v("Let's run the code.\nWhat we get is a list of numbers that appears to be in descending order right.\nThese are the values of our average last function.\nIt started from a high value and at each iteration it became lower and lower until it reached a point\nwhere it almost stopped changing.\nThis means we have minimized or almost minimize the loss function with respect to the weights and biases.\nTherefore we have found a linear function that fits the model Well")]),t._v(" "),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("113.1346113499832\n108.21425084240616\n103.88888353315217\n99.7849517016046\n95.84949440054764\n92.0702726846745\n88.4404105995388\n84.95392016265656\n81.60512775430881\n78.38859366840362\n75.29909427907384\n72.33161242080419\n69.4813290972211\n66.74361563721006\n64.11402617596718\n61.58829043492039\n59.162306787044116\n56.83213559608488\n54.59399281885322\n52.444243860188884\n50.3793976706198\n48.39610107712963\n46.491133337827655\n44.66140091167794\n42.90393243479416\n41.21587389514219\n39.594483997814095\n38.03712971334737\n36.54128200186008\n35.104511706058084\n33.72448560644502\n32.39896263232881\n31.125790222471856\n29.902900829474543\n28.728308562215652\n27.600105960897142\n26.516460899456188\n25.475613610314117\n24.475873826630895\n23.515618037423987\n22.593286851094458\n21.707382463078588\n20.856466223512808\n20.03915630096189\n19.25412543841651\n18.500098797916074\n17.77585189029656\n17.080208586701485\n16.41203920862674\n15.77025869339785\n15.153824832100188\n14.561736577101016\n13.993032416414623\n13.446788812270727\n12.922118701350518\n12.418170054254773\n11.934124491864695\n11.469195956348635\n11.022629434656377\n10.59369973242813\n10.181710296327049\n9.785992082882911\n9.40590247200998\n9.040824223434672\n8.690164474338356\n8.353353776587563\n8.029845171988013\n7.719113304060869\n7.42065356489872\n7.133981275715881\n6.858630899762206\n6.594155286322381\n6.34012494457287\n6.096127346117317\n5.861766255067884\n5.636661084584468\n5.420446278826976\n5.212770719316917\n5.013297154744328\n4.8217016532940695\n4.637673076602049\n4.460912574487216\n4.291133099638719\n4.128058941470141\n3.9714252783838333\n3.8209777477182216\n3.676472032679767\n3.537673465588702\n3.4043566467943345\n3.276305078640973\n3.153310813890127\n3.0351741180280087\n2.921703144909945\n2.812713625215003\n2.7080285672048383\n2.607477969300881\n2.5108985440130636\n2.4181334527717953\n2.3290320512325318\n2.2434496446393806\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br"),n("span",{staticClass:"line-number"},[t._v("11")]),n("br"),n("span",{staticClass:"line-number"},[t._v("12")]),n("br"),n("span",{staticClass:"line-number"},[t._v("13")]),n("br"),n("span",{staticClass:"line-number"},[t._v("14")]),n("br"),n("span",{staticClass:"line-number"},[t._v("15")]),n("br"),n("span",{staticClass:"line-number"},[t._v("16")]),n("br"),n("span",{staticClass:"line-number"},[t._v("17")]),n("br"),n("span",{staticClass:"line-number"},[t._v("18")]),n("br"),n("span",{staticClass:"line-number"},[t._v("19")]),n("br"),n("span",{staticClass:"line-number"},[t._v("20")]),n("br"),n("span",{staticClass:"line-number"},[t._v("21")]),n("br"),n("span",{staticClass:"line-number"},[t._v("22")]),n("br"),n("span",{staticClass:"line-number"},[t._v("23")]),n("br"),n("span",{staticClass:"line-number"},[t._v("24")]),n("br"),n("span",{staticClass:"line-number"},[t._v("25")]),n("br"),n("span",{staticClass:"line-number"},[t._v("26")]),n("br"),n("span",{staticClass:"line-number"},[t._v("27")]),n("br"),n("span",{staticClass:"line-number"},[t._v("28")]),n("br"),n("span",{staticClass:"line-number"},[t._v("29")]),n("br"),n("span",{staticClass:"line-number"},[t._v("30")]),n("br"),n("span",{staticClass:"line-number"},[t._v("31")]),n("br"),n("span",{staticClass:"line-number"},[t._v("32")]),n("br"),n("span",{staticClass:"line-number"},[t._v("33")]),n("br"),n("span",{staticClass:"line-number"},[t._v("34")]),n("br"),n("span",{staticClass:"line-number"},[t._v("35")]),n("br"),n("span",{staticClass:"line-number"},[t._v("36")]),n("br"),n("span",{staticClass:"line-number"},[t._v("37")]),n("br"),n("span",{staticClass:"line-number"},[t._v("38")]),n("br"),n("span",{staticClass:"line-number"},[t._v("39")]),n("br"),n("span",{staticClass:"line-number"},[t._v("40")]),n("br"),n("span",{staticClass:"line-number"},[t._v("41")]),n("br"),n("span",{staticClass:"line-number"},[t._v("42")]),n("br"),n("span",{staticClass:"line-number"},[t._v("43")]),n("br"),n("span",{staticClass:"line-number"},[t._v("44")]),n("br"),n("span",{staticClass:"line-number"},[t._v("45")]),n("br"),n("span",{staticClass:"line-number"},[t._v("46")]),n("br"),n("span",{staticClass:"line-number"},[t._v("47")]),n("br"),n("span",{staticClass:"line-number"},[t._v("48")]),n("br"),n("span",{staticClass:"line-number"},[t._v("49")]),n("br"),n("span",{staticClass:"line-number"},[t._v("50")]),n("br"),n("span",{staticClass:"line-number"},[t._v("51")]),n("br"),n("span",{staticClass:"line-number"},[t._v("52")]),n("br"),n("span",{staticClass:"line-number"},[t._v("53")]),n("br"),n("span",{staticClass:"line-number"},[t._v("54")]),n("br"),n("span",{staticClass:"line-number"},[t._v("55")]),n("br"),n("span",{staticClass:"line-number"},[t._v("56")]),n("br"),n("span",{staticClass:"line-number"},[t._v("57")]),n("br"),n("span",{staticClass:"line-number"},[t._v("58")]),n("br"),n("span",{staticClass:"line-number"},[t._v("59")]),n("br"),n("span",{staticClass:"line-number"},[t._v("60")]),n("br"),n("span",{staticClass:"line-number"},[t._v("61")]),n("br"),n("span",{staticClass:"line-number"},[t._v("62")]),n("br"),n("span",{staticClass:"line-number"},[t._v("63")]),n("br"),n("span",{staticClass:"line-number"},[t._v("64")]),n("br"),n("span",{staticClass:"line-number"},[t._v("65")]),n("br"),n("span",{staticClass:"line-number"},[t._v("66")]),n("br"),n("span",{staticClass:"line-number"},[t._v("67")]),n("br"),n("span",{staticClass:"line-number"},[t._v("68")]),n("br"),n("span",{staticClass:"line-number"},[t._v("69")]),n("br"),n("span",{staticClass:"line-number"},[t._v("70")]),n("br"),n("span",{staticClass:"line-number"},[t._v("71")]),n("br"),n("span",{staticClass:"line-number"},[t._v("72")]),n("br"),n("span",{staticClass:"line-number"},[t._v("73")]),n("br"),n("span",{staticClass:"line-number"},[t._v("74")]),n("br"),n("span",{staticClass:"line-number"},[t._v("75")]),n("br"),n("span",{staticClass:"line-number"},[t._v("76")]),n("br"),n("span",{staticClass:"line-number"},[t._v("77")]),n("br"),n("span",{staticClass:"line-number"},[t._v("78")]),n("br"),n("span",{staticClass:"line-number"},[t._v("79")]),n("br"),n("span",{staticClass:"line-number"},[t._v("80")]),n("br"),n("span",{staticClass:"line-number"},[t._v("81")]),n("br"),n("span",{staticClass:"line-number"},[t._v("82")]),n("br"),n("span",{staticClass:"line-number"},[t._v("83")]),n("br"),n("span",{staticClass:"line-number"},[t._v("84")]),n("br"),n("span",{staticClass:"line-number"},[t._v("85")]),n("br"),n("span",{staticClass:"line-number"},[t._v("86")]),n("br"),n("span",{staticClass:"line-number"},[t._v("87")]),n("br"),n("span",{staticClass:"line-number"},[t._v("88")]),n("br"),n("span",{staticClass:"line-number"},[t._v("89")]),n("br"),n("span",{staticClass:"line-number"},[t._v("90")]),n("br"),n("span",{staticClass:"line-number"},[t._v("91")]),n("br"),n("span",{staticClass:"line-number"},[t._v("92")]),n("br"),n("span",{staticClass:"line-number"},[t._v("93")]),n("br"),n("span",{staticClass:"line-number"},[t._v("94")]),n("br"),n("span",{staticClass:"line-number"},[t._v("95")]),n("br"),n("span",{staticClass:"line-number"},[t._v("96")]),n("br"),n("span",{staticClass:"line-number"},[t._v("97")]),n("br"),n("span",{staticClass:"line-number"},[t._v("98")]),n("br"),n("span",{staticClass:"line-number"},[t._v("99")]),n("br"),n("span",{staticClass:"line-number"},[t._v("100")]),n("br")])]),n("p",[t._v("The weights and the biases are optimize.\nBut so are the outputs.\nSince the optimization process has ended.\nWe can check these values here.\nWe observe the values from the last iteration of the for loop.\nThe one that gave us the lowest last function in the memory of the computer the weights biases and outputs\nvariables are optimized as of now.\nCongratulations you learn how to create your first machine learning algorithm.")]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Still let's spend an extra minute on that.\nI'd like to print the weights and the bias's the weights seem about right.\nThe bias is close to five as we wanted but not really.\nThat's because we use too few iterations or an inappropriate learning rate.\nLet's rerun the code for the loop.\nThis will continue optimizing the algorithm for another hundred iterations.\nWe can see the bias improves when we increase the number of iterations.\nWe strongly encourage you to play around with the code and find the optimal number of iterations for\nthe problem.\nTry different values for observations learning rate number of iterations maybe even initial range for\ninitializing the weights and biases cool.")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Weights: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'Biases: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("biases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br")])]),n("p",[t._v("Targets = $f(x,z) = 2x - 3z + 5 + noise$")]),t._v(" "),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("Weights: [[ 1.9962384 ]\n [-3.00212515]]\nBiases: [5.28171043]\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br")])]),n("div",{staticClass:"language- line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("Weights: [[ 1.99842328]\n [-3.00330109]]\nBiases: [5.01471378]\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br")])]),n("p",[t._v("Finally I'd like to show you the plot of the output at the last iteration against the targets.\nThe closer this plot is to a 45 degree line the closer the outputs are to the targets.\nObviously our model worked like a charm.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(353),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("h2",{attrs:{id:"solving-the-simple-example-using-tensorflow"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#solving-the-simple-example-using-tensorflow","aria-hidden":"true"}},[t._v("#")]),t._v(" Solving the simple example using TensorFlow")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. Data generation")]),t._v("\n\nobservations "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),t._v("\nxs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("low"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" high"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nzs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ngenerated_inputs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("column_stack"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nnoise "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ngenerated_targets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("xs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("zs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" noise\n\nnp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savez"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'TF_intro'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inputs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("generated_inputs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("generated_targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. Solving with TensorFlow")]),t._v("\n\ntraining_data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'TF_intro.npz'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ninput_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\noutput_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tf.keras.Sequential() function that specifies how the model")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# will be laid down ('stack layers')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Linear combination + Output = Layer*")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The tf.keras.layers.Dense(output size)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# takes the inputs provided to the model")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# and calculates the dot product of the inputs and weights and adds the bias")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# It would be the output = np.dot(inputs, weights) + bias")]),t._v("\n\nmodel "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# model.compile(optimizer, loss) configures the model for training")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# L2-norm loss = Least sum of squares (least sum of squared error)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# scaling by #observations = average (mean)")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean_squared_error'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# What we've got left is to indicate to the model which data to fit")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# modelf.fit(inouts, targets) fits (trains) the model.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Epoch = iteration over the full dataset")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epochs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. Extract the weights and bias")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nweights "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'weights: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nbiases "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'biases: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("biases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5. Extract the outputs (make predictions)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# model.predict_on_batch(data) calculates the outputs given inputs")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# these are the values that were compared to the targets to evaluate the loss function")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_on_batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("round")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("round")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6. Plotting")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The line should be as close to 45 as possible")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_on_batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Outputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br"),n("span",{staticClass:"line-number"},[t._v("11")]),n("br"),n("span",{staticClass:"line-number"},[t._v("12")]),n("br"),n("span",{staticClass:"line-number"},[t._v("13")]),n("br"),n("span",{staticClass:"line-number"},[t._v("14")]),n("br"),n("span",{staticClass:"line-number"},[t._v("15")]),n("br"),n("span",{staticClass:"line-number"},[t._v("16")]),n("br"),n("span",{staticClass:"line-number"},[t._v("17")]),n("br"),n("span",{staticClass:"line-number"},[t._v("18")]),n("br"),n("span",{staticClass:"line-number"},[t._v("19")]),n("br"),n("span",{staticClass:"line-number"},[t._v("20")]),n("br"),n("span",{staticClass:"line-number"},[t._v("21")]),n("br"),n("span",{staticClass:"line-number"},[t._v("22")]),n("br"),n("span",{staticClass:"line-number"},[t._v("23")]),n("br"),n("span",{staticClass:"line-number"},[t._v("24")]),n("br"),n("span",{staticClass:"line-number"},[t._v("25")]),n("br"),n("span",{staticClass:"line-number"},[t._v("26")]),n("br"),n("span",{staticClass:"line-number"},[t._v("27")]),n("br"),n("span",{staticClass:"line-number"},[t._v("28")]),n("br"),n("span",{staticClass:"line-number"},[t._v("29")]),n("br"),n("span",{staticClass:"line-number"},[t._v("30")]),n("br"),n("span",{staticClass:"line-number"},[t._v("31")]),n("br"),n("span",{staticClass:"line-number"},[t._v("32")]),n("br"),n("span",{staticClass:"line-number"},[t._v("33")]),n("br"),n("span",{staticClass:"line-number"},[t._v("34")]),n("br"),n("span",{staticClass:"line-number"},[t._v("35")]),n("br"),n("span",{staticClass:"line-number"},[t._v("36")]),n("br"),n("span",{staticClass:"line-number"},[t._v("37")]),n("br"),n("span",{staticClass:"line-number"},[t._v("38")]),n("br"),n("span",{staticClass:"line-number"},[t._v("39")]),n("br"),n("span",{staticClass:"line-number"},[t._v("40")]),n("br"),n("span",{staticClass:"line-number"},[t._v("41")]),n("br"),n("span",{staticClass:"line-number"},[t._v("42")]),n("br"),n("span",{staticClass:"line-number"},[t._v("43")]),n("br"),n("span",{staticClass:"line-number"},[t._v("44")]),n("br"),n("span",{staticClass:"line-number"},[t._v("45")]),n("br"),n("span",{staticClass:"line-number"},[t._v("46")]),n("br"),n("span",{staticClass:"line-number"},[t._v("47")]),n("br"),n("span",{staticClass:"line-number"},[t._v("48")]),n("br"),n("span",{staticClass:"line-number"},[t._v("49")]),n("br"),n("span",{staticClass:"line-number"},[t._v("50")]),n("br"),n("span",{staticClass:"line-number"},[t._v("51")]),n("br"),n("span",{staticClass:"line-number"},[t._v("52")]),n("br"),n("span",{staticClass:"line-number"},[t._v("53")]),n("br"),n("span",{staticClass:"line-number"},[t._v("54")]),n("br"),n("span",{staticClass:"line-number"},[t._v("55")]),n("br"),n("span",{staticClass:"line-number"},[t._v("56")]),n("br"),n("span",{staticClass:"line-number"},[t._v("57")]),n("br"),n("span",{staticClass:"line-number"},[t._v("58")]),n("br"),n("span",{staticClass:"line-number"},[t._v("59")]),n("br"),n("span",{staticClass:"line-number"},[t._v("60")]),n("br"),n("span",{staticClass:"line-number"},[t._v("61")]),n("br"),n("span",{staticClass:"line-number"},[t._v("62")]),n("br"),n("span",{staticClass:"line-number"},[t._v("63")]),n("br"),n("span",{staticClass:"line-number"},[t._v("64")]),n("br"),n("span",{staticClass:"line-number"},[t._v("65")]),n("br"),n("span",{staticClass:"line-number"},[t._v("66")]),n("br"),n("span",{staticClass:"line-number"},[t._v("67")]),n("br"),n("span",{staticClass:"line-number"},[t._v("68")]),n("br"),n("span",{staticClass:"line-number"},[t._v("69")]),n("br"),n("span",{staticClass:"line-number"},[t._v("70")]),n("br"),n("span",{staticClass:"line-number"},[t._v("71")]),n("br"),n("span",{staticClass:"line-number"},[t._v("72")]),n("br"),n("span",{staticClass:"line-number"},[t._v("73")]),n("br"),n("span",{staticClass:"line-number"},[t._v("74")]),n("br"),n("span",{staticClass:"line-number"},[t._v("75")]),n("br"),n("span",{staticClass:"line-number"},[t._v("76")]),n("br"),n("span",{staticClass:"line-number"},[t._v("77")]),n("br"),n("span",{staticClass:"line-number"},[t._v("78")]),n("br"),n("span",{staticClass:"line-number"},[t._v("79")]),n("br"),n("span",{staticClass:"line-number"},[t._v("80")]),n("br"),n("span",{staticClass:"line-number"},[t._v("81")]),n("br"),n("span",{staticClass:"line-number"},[t._v("82")]),n("br")])]),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(354),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("h3",{attrs:{id:"making-the-model-closer-to-the-numpy-example"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#making-the-model-closer-to-the-numpy-example","aria-hidden":"true"}},[t._v("#")]),t._v(" Making the model closer to the Numpy example")]),t._v(" "),n("div",{staticClass:"language-py line-numbers-mode"},[n("div",{staticClass:"highlight-lines"},[n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("div",{staticClass:"highlighted"},[t._v(" ")]),n("div",{staticClass:"highlighted"},[t._v(" ")]),n("br"),n("br"),n("div",{staticClass:"highlighted"},[t._v(" ")]),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("div",{staticClass:"highlighted"},[t._v(" ")]),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br"),n("br")]),n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. Data generation")]),t._v("\n\nobservations "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),t._v("\nxs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("low"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" high"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nzs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ngenerated_inputs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("column_stack"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nnoise "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("observations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ngenerated_targets "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("xs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("zs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" noise\n\nnp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savez"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'TF_intro'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inputs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("generated_inputs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" targets"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("generated_targets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. Solving with TensorFlow")]),t._v("\n\ntraining_data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'TF_intro.npz'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ninput_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\noutput_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tf.keras.Sequential() function that specifies how the model")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# will be laid down ('stack layers')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Linear combination + Output = Layer*")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The tf.keras.layers.Dense(output size)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# takes the inputs provided to the model")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# and calculates the dot product of the inputs and weights and adds the bias")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# It would be the output = np.dot(inputs, weights) + bias")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tf.keras.layers.Dense(output_size, kernel_initializer, bias_initializer)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# function that is laying down the model (used tp 'stack layers') and initialize weights")]),t._v("\n\nmodel "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          kernel_initializer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_uniform_initializer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("minval"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maxval"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          bias_initializer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_uniform_initializer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("minval"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" maxval"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ncustom_optimizer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optimizers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SGD"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learning_rate"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.02")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# model.compile(optimizer, loss) configures the model for training")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# L2-norm loss = Least sum of squares (least sum of squared error)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# scaling by #observations = average (mean)")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("custom_optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mean_squared_error'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# What we've got left is to indicate to the model which data to fit")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# modelf.fit(inouts, targets) fits (trains) the model.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Epoch = iteration over the full dataset")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epochs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. Extract the weights and bias")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nweights "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'weights: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nbiases "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_weights"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string-interpolation"}},[n("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'biases: ")]),n("span",{pre:!0,attrs:{class:"token interpolation"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("biases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5. Extract the outputs (make predictions)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# model.predict_on_batch(data) calculates the outputs given inputs")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# these are the values that were compared to the targets to evaluate the loss function")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_on_batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("round")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("round")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#%%")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6. Plotting")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The line should be as close to 45 as possible")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_on_batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Outputs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Targets'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br"),n("span",{staticClass:"line-number"},[t._v("11")]),n("br"),n("span",{staticClass:"line-number"},[t._v("12")]),n("br"),n("span",{staticClass:"line-number"},[t._v("13")]),n("br"),n("span",{staticClass:"line-number"},[t._v("14")]),n("br"),n("span",{staticClass:"line-number"},[t._v("15")]),n("br"),n("span",{staticClass:"line-number"},[t._v("16")]),n("br"),n("span",{staticClass:"line-number"},[t._v("17")]),n("br"),n("span",{staticClass:"line-number"},[t._v("18")]),n("br"),n("span",{staticClass:"line-number"},[t._v("19")]),n("br"),n("span",{staticClass:"line-number"},[t._v("20")]),n("br"),n("span",{staticClass:"line-number"},[t._v("21")]),n("br"),n("span",{staticClass:"line-number"},[t._v("22")]),n("br"),n("span",{staticClass:"line-number"},[t._v("23")]),n("br"),n("span",{staticClass:"line-number"},[t._v("24")]),n("br"),n("span",{staticClass:"line-number"},[t._v("25")]),n("br"),n("span",{staticClass:"line-number"},[t._v("26")]),n("br"),n("span",{staticClass:"line-number"},[t._v("27")]),n("br"),n("span",{staticClass:"line-number"},[t._v("28")]),n("br"),n("span",{staticClass:"line-number"},[t._v("29")]),n("br"),n("span",{staticClass:"line-number"},[t._v("30")]),n("br"),n("span",{staticClass:"line-number"},[t._v("31")]),n("br"),n("span",{staticClass:"line-number"},[t._v("32")]),n("br"),n("span",{staticClass:"line-number"},[t._v("33")]),n("br"),n("span",{staticClass:"line-number"},[t._v("34")]),n("br"),n("span",{staticClass:"line-number"},[t._v("35")]),n("br"),n("span",{staticClass:"line-number"},[t._v("36")]),n("br"),n("span",{staticClass:"line-number"},[t._v("37")]),n("br"),n("span",{staticClass:"line-number"},[t._v("38")]),n("br"),n("span",{staticClass:"line-number"},[t._v("39")]),n("br"),n("span",{staticClass:"line-number"},[t._v("40")]),n("br"),n("span",{staticClass:"line-number"},[t._v("41")]),n("br"),n("span",{staticClass:"line-number"},[t._v("42")]),n("br"),n("span",{staticClass:"line-number"},[t._v("43")]),n("br"),n("span",{staticClass:"line-number"},[t._v("44")]),n("br"),n("span",{staticClass:"line-number"},[t._v("45")]),n("br"),n("span",{staticClass:"line-number"},[t._v("46")]),n("br"),n("span",{staticClass:"line-number"},[t._v("47")]),n("br"),n("span",{staticClass:"line-number"},[t._v("48")]),n("br"),n("span",{staticClass:"line-number"},[t._v("49")]),n("br"),n("span",{staticClass:"line-number"},[t._v("50")]),n("br"),n("span",{staticClass:"line-number"},[t._v("51")]),n("br"),n("span",{staticClass:"line-number"},[t._v("52")]),n("br"),n("span",{staticClass:"line-number"},[t._v("53")]),n("br"),n("span",{staticClass:"line-number"},[t._v("54")]),n("br"),n("span",{staticClass:"line-number"},[t._v("55")]),n("br"),n("span",{staticClass:"line-number"},[t._v("56")]),n("br"),n("span",{staticClass:"line-number"},[t._v("57")]),n("br"),n("span",{staticClass:"line-number"},[t._v("58")]),n("br"),n("span",{staticClass:"line-number"},[t._v("59")]),n("br"),n("span",{staticClass:"line-number"},[t._v("60")]),n("br"),n("span",{staticClass:"line-number"},[t._v("61")]),n("br"),n("span",{staticClass:"line-number"},[t._v("62")]),n("br"),n("span",{staticClass:"line-number"},[t._v("63")]),n("br"),n("span",{staticClass:"line-number"},[t._v("64")]),n("br"),n("span",{staticClass:"line-number"},[t._v("65")]),n("br"),n("span",{staticClass:"line-number"},[t._v("66")]),n("br"),n("span",{staticClass:"line-number"},[t._v("67")]),n("br"),n("span",{staticClass:"line-number"},[t._v("68")]),n("br"),n("span",{staticClass:"line-number"},[t._v("69")]),n("br"),n("span",{staticClass:"line-number"},[t._v("70")]),n("br"),n("span",{staticClass:"line-number"},[t._v("71")]),n("br"),n("span",{staticClass:"line-number"},[t._v("72")]),n("br"),n("span",{staticClass:"line-number"},[t._v("73")]),n("br"),n("span",{staticClass:"line-number"},[t._v("74")]),n("br"),n("span",{staticClass:"line-number"},[t._v("75")]),n("br"),n("span",{staticClass:"line-number"},[t._v("76")]),n("br"),n("span",{staticClass:"line-number"},[t._v("77")]),n("br"),n("span",{staticClass:"line-number"},[t._v("78")]),n("br"),n("span",{staticClass:"line-number"},[t._v("79")]),n("br"),n("span",{staticClass:"line-number"},[t._v("80")]),n("br"),n("span",{staticClass:"line-number"},[t._v("81")]),n("br"),n("span",{staticClass:"line-number"},[t._v("82")]),n("br"),n("span",{staticClass:"line-number"},[t._v("83")]),n("br"),n("span",{staticClass:"line-number"},[t._v("84")]),n("br"),n("span",{staticClass:"line-number"},[t._v("85")]),n("br"),n("span",{staticClass:"line-number"},[t._v("86")]),n("br"),n("span",{staticClass:"line-number"},[t._v("87")]),n("br"),n("span",{staticClass:"line-number"},[t._v("88")]),n("br")])]),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(355),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(356),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("h2",{attrs:{id:"going-deeper-introduction-to-deep-neural-networks"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#going-deeper-introduction-to-deep-neural-networks","aria-hidden":"true"}},[t._v("#")]),t._v(" Going deeper Introduction to deep neural networks")]),t._v(" "),n("p",[t._v("Mixing linear combinations and non-linearities allows us to model arbitrary functions.")]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("The layer is the building block of neural networks.\nThe initial linear combinations and the added non-linearity form a layer.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(357),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("When we have more then one layer, we are talking about neural network.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(358),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(359),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(360),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(361),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(362),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("We refer to the width and depth (but not only) as hyperparameters")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(363),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(364),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Each arrow represents the mathematical transformation of a certain value")]),t._v(" "),n("p",[t._v("So a certain way is applied then a non-linearity is added know that the non-linearity doesn't change\nthe shape of the expression it only changes its linearity.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(365),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("For example weight 3 6 is applied to the third input and is involved in calculating the 6th hidden unit in the same way,\nWeights 1 6 2 6 and so on until 8 6 all participate in computing the sixth hit and unit.\n"),n("strong",[t._v("They are linearly combined")]),t._v(".\nAnd then "),n("strong",[t._v("nonlinearity is added")]),t._v(" in order to produce the sixth hidden unit in the same way we get each of the\nother hidden units.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(366),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(367),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Well then we have the first hit and we're using the same logic we can linearly combine the hidden units\nand apply a nonlinearity right.\nIndeed this time though there are nine input hit in units and none output hit in units.\nTherefore the weights will be contained in a 9 by 9 matrix and there will be 81 arrows.\nFinally we applied nonlinearity and we reached the second hidden layer.\nWe can go on and on and on like this we can add a hundred hidden layers if we want.\nThat's a question of how deep we want our deep net to be.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(368),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Finally we'll have the last hidden layer when we apply the operation once again we will reach the output\nlayer the output units depend on the number of outputs we would like to have in this picture.\nThere are four.\nThey may be the temperature humidity precipitation and pressure for the next day.\nTo reach this point we will have a nine by four Waite's matrix which refers to 36 arrows or 36 weights\nexactly what we expected.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(369),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("All right as before our optimization goal is finding values for major cities that would allow us to\nconvert inputs into correct outputs as best as we can.\nThis time though we are not using a single linear model but a complex infrastructure with a much higher\nprobability of delivering a meaningful result.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(370),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("We said non-linearities are needed so we can represent more complicated relationships.\nWhile that is true it isn't the full picture an important consequence of including non-linearities is "),n("strong",[t._v("the ability to stack Layers stacking layers is the process of placing one layer after the other in a meaningful way. Remember that it's fundamental.")])]),t._v(" "),n("br"),t._v(" "),n("p",[n("strong",[t._v("We cannot stack layers when we only have linear relationships.")])]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(323),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(324),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("The point we will make is that we cannot stack Lears when we have only linear relationships.\nLet's prove it.\nImagine we have a single hidden layer and there are no non-linearities.\nSo our picture looks this way.\nThere are eight input nodes nine head and nodes in the hidden layer and four output nodes.\nTherefore we have an eight by nine Waites matrix.\nWhen your relationship between the input layer and the hidden layer Let's call this matrix W. one.\nThe hidden units age according to the linear model H is equal to x times w 1.\nLet's ignore the biases for a while.\nSo our hidden units are summarized in the matrix H with a shape of one by nine.\nNow let's get to the output layer from the hidden layer once again according to the linear model Y is\nequal to h times W2 we have W2 as these weights are different.\nWe already know the H matrix is equal to x times.\nW1 Right.\nLet's replace h in this equation Y is equal to x times w 1 times.\nW2 but w 1 and w 2 can be multiplied right.\nWhat we get is a combined matrix W star with dimensions 8 by 4 well then our deep net can be simplified\ninto a linear model which looks this way y equals x times w star knowing that we realize the hidden\nlayer is completely useless in this case.\nWe can just train this simple linear model and we would get the same result in mathematics.\nThis seems like an obvious fact but in machine learning it is not so clear from the beginning.\nThe two consecutive linear transformations are equivalent to a single one.\nEven if we add 100 layers the problem would be simplified to a single transformation.\nThat is the reason we need non-linearities.\nWithout them stacking layers one after the other is meaningless and without stacking layers we will\nhave no depth.\nWhat's more with no depth.\nEach and every problem will equal the simple linear example we did earlier.\nAnd many practitioners would tell you it was borderline machine learning.\nAll right let's summarize in one sentence.")]),t._v(" "),n("br"),t._v(" "),n("p",[n("strong",[t._v("You have deep nets and find complex relationships through arbitrary functions.\nWe need non-linearities.")])]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(371),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Point taken.")]),t._v(" "),n("p",[t._v("Non-linearities are also called activation functions.\nHenceforth that's how we will refer to them activation functions transform inputs into outputs of a different kind.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(372),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Think about the temperature outside.\nI assume you wake up and the sun is shining.\nSo you put on some light clothes you go out and feel warm and comfortable.\nYou carry your jacket in your hands in the afternoon the temperature starts decreasing.\nInitially you don't feel a difference at some point though your brain says it's getting cold.\nYou listen to your brain and put on your jacket the input you got was the change in the temperature\nthe activation function transformed this input into an action put on the jacket or continue carrying\nit.\nThis is also the output after the transformation.\nIt is a binary variable jacket or no jacket.\nThat's the basic logic behind nonlinearities the change in the temperature was following a linear model")]),t._v(" "),n("p",[n("strong",[t._v("as it was steadily decreasing the activation function transformed this relationship into an output linked\nto the temperature but was of a different kind")])]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(373),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("its derivative as you may recall the derivative is an essential part of the gradient\ndescent.\nNaturally when we work with tenths or flow we won't need to calculate the derivative as tenths or flow.\nDoes that automatically Anyhow the purpose of this lesson is understanding these functions.\nThere are graphs and ranges in a way that would allow us to acquire intuition about the way they behave.\nHere's the functions graph.\nAnd finally we have it's range.\nOnce we have applied the sigmoid as an activator all the outputs will be contained in the range from\n0 to 1.\nSo the output is somewhat standardized.\nAll right here are the other three common activators the Tench also known as the hyperbolic tangent.\nThe real Lu aka the rectified linear unit and the soft Max activator you can see their formulas derivatives\ngraphs and ranges.\nThe saaf next graph is not missing.\nThe reason we don't have it here is that it is different every time.\nPause this video for a while and examine the table in more detail.\nYou can also find this table in the course notes.\nSo all these functions are activators right.\nWhat makes them similar.\nWell let's look at their graphs all Armano tonic continuous and differentiable.\nThese are important properties needed for the optimization process as we are not there yet.\nWe will leave this issue for later.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(325),alt:"Training Data - Example 1"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Before we conclude I would like to make this remark.\nActivation functions are also called transfer functions because of the transformation properties.\nThe two terms are used interchangeably in machine learning context but have differences in other fields.\nTherefore to avoid confusion we will stick to the term activation functions.")]),t._v(" "),n("h3",{attrs:{id:"softmax-function"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#softmax-function","aria-hidden":"true"}},[t._v("#")]),t._v(" softmax function")]),t._v(" "),n("p",[t._v("Let's continue exploring this table which contains mostly Greek letters we said the soft max function\nhas no definite graph y so while this function is different if we take a careful look at its formula\nwe would see the key difference between this function and the other is it takes an argument the whole\nvector A instead of individual elements.\nSo the self max function is equal to the exponential of the element at position.\nI divided by the sum of the exponentials of all elements of the vector.\nSo while the other activation functions get an input value and transform it regardless of the other\nelements the SAAF Max considers the information about the whole set of numbers we have.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(374),alt:"Softmax"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("A key aspect of the soft Max transformation is that the values it outputs are in the range from 0 to 1.\nThere is some is exactly 1 What else has such a property.\nProbabilities Yes probabilities indeed.\nThe point of the soft Max transformation is to transform a bunch of arbitrarily large or small numbers\nthat come out of previous layers and fit them into a valid probability distribution.\nThis is extremely important and useful.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(375),alt:"Softmax"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Remember our example with cats dogs and horses we saw earlier.\nOne photo was described by a vector containing 0.1 0.2 and 0.7.\nWe promise we will tell you how to do that.\nWell that's how through a soft Max transformation we kept our promise now that we know we are talking\nabout probabilities we can comfortably say we are 70 percent certain the image is a picture of a horse.\nThis makes everything so intuitive and useful that the SAAF next activation is often used as the activation\nof the final output layer and classification problems.\nSo no matter what happens before the final output of the algorithm is a probability distribution.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(376),alt:"Softmax"}})]),t._v(" "),n("br"),t._v(" "),n("h3",{attrs:{id:"backpropagation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#backpropagation","aria-hidden":"true"}},[t._v("#")]),t._v(" Backpropagation")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(377),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("First I'd like to recap what we know so far we've seen and understood the logic of how layers are stacked.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(378),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("We've also explored a few activation functions and spent extra time showing they are central to the concept of stacking layers.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(325),alt:"DeepNet"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(323),alt:"DeepNet"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(324),alt:"DeepNet"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Moreover by now we have said 100 times that the training process consists of updating parameters through the gradient descent for optimizing the objective function.")]),t._v(" "),n("p",[t._v("In supervised learning the process of optimization consisted of minimizing the loss.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(379),alt:"Gradient Descent"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Our updates were directly related to the partial derivatives of the loss and indirectly related to the")]),t._v(" "),n("p",[t._v("errors or deltas as we called them.")]),t._v(" "),n("p",[t._v("Let me remind you that the Deltas were the differences between the targets and the outputs.")]),t._v(" "),n("p",[t._v("All right as we will see later deltas for the hidden layers are trickier to define.\nStill they have a similar meaning.")]),t._v(" "),n("p",[t._v("The procedure for calculating them is called back propagation of errors having these deltas allows us to vary parameters using the familiar update rule.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(380),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Let's start from the other side of the coin forward propagation")]),t._v(" "),n("p",[t._v("Forward propagation is the process of pushing inputs through the net.")]),t._v(" "),n("p",[t._v("At the end of each epoch the obtained outputs are compared to the targets to form the errors.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(381),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Then we back propagate through partial derivatives and change each parameter so errors at the next epoch are minimized.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(382),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("For the minimal example the back propagation consisted of a single step, aligning the weights, given the errors we obtained.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(383),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Here's where it gets a little tricky when we have a deep net.\nWe must update all the weights related to the input layer and the hidden layers.\nFor example in this famous picture we have 270 weights and yes this means we had to manually draw all 270 arrows you see here.")]),t._v(" "),n("p",[t._v("So updating all 270 weights is a big deal.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(384),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("But wait.\nWe also introduced activation functions.\nThis means we have to update the weights accordingly.\nConsidering the use nonlinearities and their derivatives.")]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Finally to update the weights we must compare the outputs to the targets.\nThis is done for each layer but we have no targets for the hidden units.\nWe don't know the errors So how do we update the weights.\nThat's what back propagation is all about.\nWe must arrive the appropriate updates as if we had targets.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(385),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Now the way academics solve this issue is through errors.\nThe main point is that we can trace the contribution of each unit hit or not to the error of the output.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(326),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("OK great let's look at the schematic illustration of back propagation shown here our net is quite simple.")]),t._v(" "),n("p",[t._v("It has a single hidden layer.")]),t._v(" "),n("p",[t._v("Each node is labeled.")]),t._v(" "),n("p",[t._v("So we have inputs x 1 and x 2 hidden layer units output layer units.\nWhy one in y2 two.\nAnd finally the targets T1 and T2.")]),t._v(" "),n("p",[t._v("The weights are w 1 1 w 1 2 w 1 3 w 2 1 w 2 2 and W 2 3 for the first part of the net.\nFor the second part we named them you 1 1 you 1 2 you 2 1 you 2 2 3 1 and you 3 2.\nSo we can differentiate between the two types of weights.")]),t._v(" "),n("p",[t._v("That's very important.")]),t._v(" "),n("p",[t._v("We know the error associated with Y 1 and y to as it depends on known targets.\nSo let's call the two errors.\nE 1 and 2.")]),t._v(" "),n("p",[t._v("Based on them we can adjust the weights labeled with you.\nEach U contributes to a single error.")]),t._v(" "),n("p",[t._v("For example you 1 1 contributes to e 1.")]),t._v(" "),n("p",[t._v("Then we find its derivative and update the coefficient.\nNothing new here.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(386),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Now let's examine w 1 1 .\nHelped us predict h1 But then we needed h1 to calculate y1 in y2.\nThus it played a role in determining both errors.\ne1 and e2.")]),t._v(" "),n("p",[t._v("So while u11 contributes to a single error w11 contributes to both errors.\nTherefore it's adjustment rule must be different.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(387),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("The solution to this problem is to take the errors and back propagate them through the net using the\nweights.")]),t._v(" "),n("p",[t._v("Knowing the u weights we can measure the contribution of each hit in unit to the respective errors.")]),t._v(" "),n("p",[t._v("Then once we found out the contribution of each hit in unit to the respective errors we can update the\nW weights.")]),t._v(" "),n("p",[t._v("So essentially through back propagation the algorithm identifies which weights lead to which errors\nthen it adjusts the weights that have a bigger contribution to the errors by more than the weights with\na smaller contribution.")]),t._v(" "),n("p",[t._v("A big problem arises when we must also consider the activation functions.\nThey introduce additional complexity to this process.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(388),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Linear your contributions are easy but non-linear ones are tougher.\nEmergent back propagating in our introductory net.\nOnce you understand it, it seems very simple.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(326),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("While pictorially straightforward mathematically it is rough to say the least.")]),t._v(" "),n("p",[t._v("That is why back propagation is one of the biggest challenges for the speed of an algorithm.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(389),alt:"Backpropagation"}})]),t._v(" "),n("br"),t._v(" "),n("h2",{attrs:{id:"overfitting"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#overfitting","aria-hidden":"true"}},[t._v("#")]),t._v(" Overfitting")]),t._v(" "),n("table",[n("thead",[n("tr",[n("th",{staticStyle:{"text-align":"center"}},[t._v("Underfitting")]),t._v(" "),n("th",{staticStyle:{"text-align":"center"}},[t._v("Overfitting")])])]),t._v(" "),n("tbody",[n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("Underfeeding means the model has not captured the underlying logic of the data. It doesn't know what to do and therefore provides an answer that is far from correct.")]),t._v(" "),n("td",{staticStyle:{"text-align":"center"}},[t._v('Broadly speaking overfitting means our training has focused on the particular training set so much it has "missed the point".')])])])]),t._v(" "),n("p",[t._v("First we will look at a regression and then we'll consider a classification problem.\nHere we can see several data points following the blue function with some minor noise a good algorithm would result in a model that looks like this.")]),t._v(" "),n("p",[t._v("It is not perfect but it is very close to the actual relationship.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(390),alt:"Fit Model"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("We can certainly say a linear model would be an underfeeding model.\nIt provides an answer but does not capture the underlying logic of the data.\nIt doesn't have strong predictive power.\nIt's kind of lame under fitted models are clumsy.")]),t._v(" "),n("p",[t._v("They have high costs in terms of high loss functions and their accuracy is low.\nYou quickly realize that either there are no relationships to be found or you need a more complex model.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(391),alt:"Underfitted Model"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Let's check in overfitting model here it is now that you see this picture we can say overfitting refers to models that are so super good at modeling the training data that they fit or come very near each observation.")]),t._v(" "),n("p",[t._v("The problem is that the random noise is captured inside and overfitting model.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(392),alt:"Underfitted Model"}})]),t._v(" "),n("br"),t._v(" "),n("h3",{attrs:{id:"summary"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#summary","aria-hidden":"true"}},[t._v("#")]),t._v(" Summary")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(393),alt:"Underfitted Model"}})]),t._v(" "),n("br"),t._v(" "),n("h2",{attrs:{id:"training-and-validation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#training-and-validation","aria-hidden":"true"}},[t._v("#")]),t._v(" Training and validation")]),t._v(" "),n("p",[t._v("Usually we'll be able to spot overfitting by dividing our available data into three subsets training validation and test.")]),t._v(" "),n("p",[t._v("The first one is the training data set as its name suggests.")]),t._v(" "),n("p",[t._v("It helps us train the model to its final form.\nAs you should know that's the place where we perform everything we've seen until now nothing is new here since so far we thought all data is training data but we intentionally labelled the Python variables training data instead of data.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(394),alt:"Training Validation and Test"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("The validation data set is the one that will help us detect and prevent overfitting.")]),t._v(" "),n("p",[t._v("All the training is done on the training set.\nIn other words we update the weights for the training so only every once in a while we stop training for a bit.\nAt this point the model is somewhat trained.\nWhat we do next is take the model and apply it to the validation data set.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(395),alt:"Training"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("This time we just run it without updating the weights so we only propagate forward not backward.")]),t._v(" "),n("p",[t._v("In other words we just calculate its loss function on average the last function calculated for the validation set should be the same as the one of the training set.")]),t._v(" "),n("p",[t._v("This is logical as the training and validation sets were extracted from the same initial dataset containing the same perceived dependencies.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(396),alt:"Validation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("Normally we would perform this operation many times in the process of creating a good machine learning algorithm.")]),t._v(" "),n("p",[t._v("The two last functions we calculate are referred to as training loss and validation loss and because the data in the training is trained using the gradient descent.")]),t._v(" "),n("p",[t._v("Each subsequent loss will be lower or equal to the previous one.")]),t._v(" "),n("p",[t._v("That's how gradient descent works by definition so we are sure that treating loss is being minimized.")]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(397),alt:"Training and Validation"}})]),t._v(" "),n("br"),t._v(" "),n("p",[t._v("That's where the validation loss comes in play at some point the validation loss could start increasing.")]),t._v(" "),n("p",[n("strong",[t._v("That's a red flag.")])]),t._v(" "),n("p",[t._v("We are overfitting we are getting better at predicting the training set but we are moving away from the overall logic data.")]),t._v(" "),n("p",[n("strong",[t._v("At this point we should stop training the model.")])]),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(398),alt:"Overfitting Flag"}})]),t._v(" "),n("br"),t._v(" "),n("br"),t._v(" "),n("div",{staticStyle:{"text-align":"center"}},[n("img",{attrs:{src:a(399),alt:"Validation Loss Vs Training Loss"}})]),t._v(" "),n("br"),t._v(" "),n("p",[n("strong",[t._v("It is extremely important that the model is not trained on validation samples.")])]),t._v(" "),n("p",[t._v("This will eliminate the whole purpose of the above mentioned process.\nThe training set and the validation set should be separate without overlapping each other.")])])}),[],!1,null,null,null);s.default=e.exports}}]);